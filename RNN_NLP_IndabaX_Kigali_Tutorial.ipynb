{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLb3FfKrX6WD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Ao7A0uxtAL7d",
    "outputId": "65dd9240-441c-4c2b-df11-2e12b6d05857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "QD6_TWPtTDVp",
    "outputId": "4a1317fe-3ef4-4375-f680-229be857253f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/76/89dd44458eb976347e5a6e75eb79fecf8facd46c1ce259bad54e0044ea35/tensorboardX-1.6-py2.py3-none-any.whl (129kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (40.9.0)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-1.6\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfFm3Ey4DbUS"
   },
   "source": [
    "# Part 0 : Introduction to RNN and GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lv43udJssC_j"
   },
   "source": [
    " The main limitations of Feed Forward Neural Networks (and also Convolutional Networks) are:\n",
    "  \n",
    "  * They accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). \n",
    "  \n",
    "  * These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model, number of neurons per layer). \n",
    "  \n",
    " Feedforward networks are amnesiacs regarding their recent past; they remember nostalgically only the formative moments of training.\n",
    "\n",
    " <img src=\"img/RNN3.png\" />\n",
    "\n",
    "Recurrent networks, on the other hand, take as their input not just the current input example they see, but also what they have perceived previously in time. \n",
    "\n",
    "For example, when we say a person is haunted by their deeds, we are simply talking about the consequences that past outputs wreak on present time. The French call this “Le passé qui ne passe pas,” or “The past that does not pass away.”\n",
    "\n",
    "The decision a recurrent net reached at time step t-1 affects the decision it will reach one moment later at time step t. So recurrent networks have two sources of input, the present and the recent past. It is often said that recurrent networks have memory which allows them to perform tasks that feedforward networks can’t.\n",
    "  \n",
    "Mathematically, recurrent neural net can be written as:  \n",
    "\n",
    "$H_t = \\sigma(U*X_t + W*H_{t-1})$\n",
    "$y_t = \\sigma(V*H_{t-1})$\n",
    "\n",
    "U: weight vector associated to the $x_t$\n",
    "\n",
    "W:  weight vector associated to the  hidden layer\n",
    "\n",
    "V: weight vector associated to the output layer\n",
    "\n",
    "The hidden state at time step t is h_t. It is a function of the input at the same time step $x_t$ and  the hidden state of the previous time step $ h_{t-1}$ The function  $\\phi$ can be the sigmoid function($\\sigma$) or tangent hyperbolic($tanh$)\n",
    "\n",
    "$y_t$ represente the output at time step t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0_yHo-3mKQi"
   },
   "source": [
    "###  Limitations of RNN:\n",
    "\n",
    "During back propagation, recurrent neural networks suffer from the vanishing  gradient problem which means when updating the model parameters the gradient can goes to zeros.\n",
    "\n",
    "more about backpropagation and vanishing gradient: http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/,  https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HDOeSCtZmO_v"
   },
   "source": [
    "### GRU\n",
    "\n",
    "GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. \n",
    "\n",
    "To solve that problem , GRU uses, so called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output.\n",
    "\n",
    "## Update Gate:\n",
    "\n",
    "$z_t = \\sigma(W^{(z)}x_t + U^{(z)}h_{t-1})$\n",
    "\n",
    "$x_t$:  current input\n",
    "\n",
    "$W^{(z)}$ : weight associated to $x_t$\n",
    "\n",
    "$h_{t-1}$ : output of the previous unit\n",
    "\n",
    "$U^{(z)}$: weight associated to $h_{t-1}$\n",
    "\n",
    "The update gate helps the model to determine how much of the past information  needs to be passed along to the future. \n",
    "Suppose we have: \n",
    "### 'Hello, my name is Jean  and I'm from France. I speak....'\n",
    "\n",
    "To predict the next word we don't need to remember all the previous part. We can just use 'I'm from France' to now that the word to predict is 'French'. That's exactly what the update gate does, it allow the network to jus remember the usefull part to predict the next output.\n",
    "\n",
    "That allows the model to decide to copy all the information from the past and eliminate the risk of vanishing gradient problem\n",
    "\n",
    "## Reset Gate:\n",
    "\n",
    "$r_t = \\sigma(W^{(r)}x_t + U^{(r)}h_{t-1})$\n",
    "\n",
    "this gate is used from the model to decide how much of the past information to forget. The difference  between Update Gate and Reset Gate comes in the weights and the gate’s usage.\n",
    "\n",
    "Coming back to the example above the reset gate tell to the model you don't need to remember  'Hello, my name is Jean  and' to predict the next word.\n",
    "\n",
    "## Current memory content\n",
    "\n",
    "This will use the reset gate to store the relevant information from the past.\n",
    "\n",
    "$h_{t}^{\\prime} = \\tanh(Wx_t +r_t ⊙  Uh_{t-1})$ \n",
    "\n",
    "$r_t ⊙  Uh_{t-1} $:  Hadamard (element-wise) product between the reset gate $r_t$ and $Uh_{t-1}$. That will determine what to remove from the previous time steps\n",
    "\n",
    "$\\tanh$: nonlinear activation function\n",
    "\n",
    "## Final memory at current time step:\n",
    "\n",
    "In this step the update gate is used.  It determines what to collect from the current memory content  $h_{t}^{\\prime}$ and what from the previous steps  $ h_{t-1}$\n",
    "\n",
    "$h_t = z_t  ⊙ h_{t-1} + (1-z_t)  ⊙ h_{t}^{\\prime}$\n",
    "\n",
    "* Apply element-wise multiplication to the update gate $z_t$ and $h_{t-1}$.\n",
    "* Apply element-wise multiplication to $(1-z_t) $and $h_{t}^{\\prime}$.\n",
    "* Sum the two results \n",
    "\n",
    "\n",
    "Putting all in a graph GRU looks like the schema below:\n",
    "\n",
    " <img src=\"img/GRU1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZXFOgLTmhd3"
   },
   "source": [
    "\n",
    "more details [towardsdatascience](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be), [medium](https://medium.com/ai-journal/lstm-gru-recurrent-neural-networks-81fe2bcdf1f9), [karpathy blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "more references:\n",
    "\n",
    "https://skymind.ai/wiki/lstm\n",
    "\n",
    "https://medium.com/@carynmccarthy15/a-beginners-guide-to-recurrent-neural-networks-bfacb27bddb6\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "https://towardsdatascience.com/what-is-a-recurrent-nns-and-gated-recurrent-unit-grus-ea71d2a05a69\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQjdaHwbMn3G"
   },
   "source": [
    "# Part 2 : Question Answering with ParlAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UyRBdZeYzrA5"
   },
   "source": [
    "# Introduction to Question Answering \n",
    "\n",
    "Search engines like Google and Yahoo! that allow the users to search for documents on the World\n",
    "Wide Web. In Search engines the user has to check each and every document to get useful answer to the\n",
    "question and it is a time consuming process. **The Question Answering (QA) system** reduces the search time toget exact answer to the question. Question Answering system is an important research area in information\n",
    "retrieval. Research on the area of Question Answering system started in the year 1960 and present lot of\n",
    "Question Answering systems have been developed. Question Answering system combines the research from\n",
    "different domains like Natural Language Processing, Artificial Intelligence, Information Retrieval and\n",
    "Information extraction.\n",
    "\n",
    "**The objective** of question answering system is to find exact answer to the question asked\n",
    "by user in natural language\n",
    "\n",
    "To understand the Question Answering subject, we firstly define the associated terms. \n",
    "\n",
    "*   **A Question Phrase** is the part of the question that says what is searched.\n",
    "*   ** Question Type**  refers to a categorization of the question for its purpose.\n",
    "*   ** Answer Type** refers to a class of objects which are sought by the question. \n",
    "*    **Question Focus** is the property or entity being searched by the question. \n",
    "*   **Question Topic** is the object or event that the question is about. \n",
    "*  **Candidate Passage** can broadly be defined as anything from a sentence to a document retrieved by a search engine in response to a question. \n",
    "* ** Candidate Answer** is the text ranked according to its suitability to as an answer\n",
    "\n",
    "\n",
    "But here we are not going to check all this details, in this case we just care about the  passage, question and answer because our data set contain a passages and  each passage is following by question and answer.\n",
    "\n",
    "Generally, the question answering system can be\n",
    "classified into two domains:\n",
    "*  **Closed domain question answering system**\n",
    "\n",
    "  deals with questions under a specific\n",
    "domain and can be seen as an easier task because NLP\n",
    "systems can exploit domain-specific knowledge frequently\n",
    "formalized in ontologies.  Alternatively, closed-domain might\n",
    "refer to a situation where only a limited type of questions are\n",
    "accepted, such as questions asking for descriptive rather\n",
    "than procedural information.\n",
    "* ** Open domain question answering system** \n",
    "\n",
    " It is deals with questions about nearly anything, and\n",
    "can only rely on general ontologies and world knowledge.\n",
    "\n",
    "Closed domain question answering system give more exact and correct answer than the open domain question answering system.\n",
    "\n",
    "# Exampe of Q/A:\n",
    "\n",
    "* This example from our data set (Babi tasks) which call Three supporting facts that is mean to answer the question we use the first three sentence\n",
    "\n",
    "**Task 3: Three Supporting Facts **  \n",
    "John picked up the apple.  \n",
    "John went to the office.  \n",
    "John went to the kitchen.  \n",
    "John dropped the apple.   \n",
    "Where was the apple before the kitchen?  \n",
    "**Answer: office**\n",
    "\n",
    "* Example of two supporting facts, we use the two first sentence to answer the question\n",
    "\n",
    "**Task 2: Two Supporting Facts**  \n",
    "John is in the playground.  \n",
    "John picked up the football.  \n",
    "Bob went to the kitchen.  \n",
    "Where is the football?  \n",
    "**Answer: playground**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Real application of Question answering\n",
    "\n",
    "* Question answering use in any website to retrive information as example Google.\n",
    "\n",
    "* Chatbots is using Q/A to make a conversation between the user and agent.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "1. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=11&cad=rja&uact=8&ved=2ahUKEwit9rWHls_hAhXKz4UKHUvWCscQFjAKegQIAxAB&url=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F320978810_An_Overview_of_Question_Answering_System&usg=AOvVaw01Y_S9X-3MXMpyrFg8mVqL\n",
    "2. https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=13&cad=rja&uact=8&ved=2ahUKEwit9rWHls_hAhXKz4UKHUvWCscQFjAMegQIAhAB&url=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3825096%2F&usg=AOvVaw04BMj_m3ELsoau6ylXl6Pn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_QU_pewNEAC"
   },
   "source": [
    "### ParlAI\n",
    "[ParlAI](https://github.com/facebookresearch/ParlAI/blob/master/README.md) (pronounced “par-lay”) is a framework for dialogue AI research, implemented in Python.\n",
    "\n",
    "Its goal is to provide researchers:\n",
    "\n",
    "* a unified framework for sharing, training and testing dialogue models\n",
    "* many popular datasets available all in one place -- from open-domain chitchat to visual question answering.\n",
    "* a wide set of reference models -- from retrieval baselines to Transformers.\n",
    "* seamless integration of Amazon Mechanical Turk for data collection and human evaluation\n",
    "* integration with Facebook Messenger to connect agents with humans in a chat interface\n",
    "\n",
    "Documentation can be found [here](http://www.parl.ai/static/docs/), some of this tutorial is inspired from the ParlAI documentation so feel free to go back and forth between the notebook and the documentation.\n",
    "\n",
    "\n",
    "### Setup the notebook\n",
    "If using google colab, make sure to use TPU runtime by going to ***Runtime > Change runtime type > Hardware accelerator: TPU > Save***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqNqCH3XNOp-"
   },
   "source": [
    "### Install ParlAI\n",
    "\n",
    "Start by installing ParlAI from github. The ParlAI folder will be located in the home directory at `~/ParlAI/`.  \n",
    "*Note: In a jupyter notebook, you can run arbitrary bash commands by prefixing them with a question mark, example: `!echo \"Hello World\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "S0YKXmMUNe_C",
    "outputId": "b8eb56a1-57a3-4d74-cda4-9dd5a14437dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/root/ParlAI'...\n",
      "remote: Enumerating objects: 40, done.\u001b[K\n",
      "remote: Counting objects:   2% (1/40)   \u001b[K\r",
      "remote: Counting objects:   5% (2/40)   \u001b[K\r",
      "remote: Counting objects:   7% (3/40)   \u001b[K\r",
      "remote: Counting objects:  10% (4/40)   \u001b[K\r",
      "remote: Counting objects:  12% (5/40)   \u001b[K\r",
      "remote: Counting objects:  15% (6/40)   \u001b[K\r",
      "remote: Counting objects:  17% (7/40)   \u001b[K\r",
      "remote: Counting objects:  20% (8/40)   \u001b[K\r",
      "remote: Counting objects:  22% (9/40)   \u001b[K\r",
      "remote: Counting objects:  25% (10/40)   \u001b[K\r",
      "remote: Counting objects:  27% (11/40)   \u001b[K\r",
      "remote: Counting objects:  30% (12/40)   \u001b[K\r",
      "remote: Counting objects:  32% (13/40)   \u001b[K\r",
      "remote: Counting objects:  35% (14/40)   \u001b[K\r",
      "remote: Counting objects:  37% (15/40)   \u001b[K\r",
      "remote: Counting objects:  40% (16/40)   \u001b[K\r",
      "remote: Counting objects:  42% (17/40)   \u001b[K\r",
      "remote: Counting objects:  45% (18/40)   \u001b[K\r",
      "remote: Counting objects:  47% (19/40)   \u001b[K\r",
      "remote: Counting objects:  50% (20/40)   \u001b[K\r",
      "remote: Counting objects:  52% (21/40)   \u001b[K\r",
      "remote: Counting objects:  55% (22/40)   \u001b[K\r",
      "remote: Counting objects:  57% (23/40)   \u001b[K\r",
      "remote: Counting objects:  60% (24/40)   \u001b[K\r",
      "remote: Counting objects:  62% (25/40)   \u001b[K\r",
      "remote: Counting objects:  65% (26/40)   \u001b[K\r",
      "remote: Counting objects:  67% (27/40)   \u001b[K\r",
      "remote: Counting objects:  70% (28/40)   \u001b[K\r",
      "remote: Counting objects:  72% (29/40)   \u001b[K\r",
      "remote: Counting objects:  75% (30/40)   \u001b[K\r",
      "remote: Counting objects:  77% (31/40)   \u001b[K\r",
      "remote: Counting objects:  80% (32/40)   \u001b[K\r",
      "remote: Counting objects:  82% (33/40)   \u001b[K\r",
      "remote: Counting objects:  85% (34/40)   \u001b[K\r",
      "remote: Counting objects:  87% (35/40)   \u001b[K\r",
      "remote: Counting objects:  90% (36/40)   \u001b[K\r",
      "remote: Counting objects:  92% (37/40)   \u001b[K\r",
      "remote: Counting objects:  95% (38/40)   \u001b[K\r",
      "remote: Counting objects:  97% (39/40)   \u001b[K\r",
      "remote: Counting objects: 100% (40/40)   \u001b[K\r",
      "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
      "remote: Compressing objects:   3% (1/32)   \u001b[K\r",
      "remote: Compressing objects:   6% (2/32)   \u001b[K\r",
      "remote: Compressing objects:   9% (3/32)   \u001b[K\r",
      "remote: Compressing objects:  12% (4/32)   \u001b[K\r",
      "remote: Compressing objects:  15% (5/32)   \u001b[K\r",
      "remote: Compressing objects:  18% (6/32)   \u001b[K\r",
      "remote: Compressing objects:  21% (7/32)   \u001b[K\r",
      "remote: Compressing objects:  25% (8/32)   \u001b[K\r",
      "remote: Compressing objects:  28% (9/32)   \u001b[K\r",
      "remote: Compressing objects:  31% (10/32)   \u001b[K\r",
      "remote: Compressing objects:  34% (11/32)   \u001b[K\r",
      "remote: Compressing objects:  37% (12/32)   \u001b[K\r",
      "remote: Compressing objects:  40% (13/32)   \u001b[K\r",
      "remote: Compressing objects:  43% (14/32)   \u001b[K\r",
      "remote: Compressing objects:  46% (15/32)   \u001b[K\r",
      "remote: Compressing objects:  50% (16/32)   \u001b[K\r",
      "remote: Compressing objects:  53% (17/32)   \u001b[K\r",
      "remote: Compressing objects:  56% (18/32)   \u001b[K\r",
      "remote: Compressing objects:  59% (19/32)   \u001b[K\r",
      "remote: Compressing objects:  62% (20/32)   \u001b[K\r",
      "remote: Compressing objects:  65% (21/32)   \u001b[K\r",
      "remote: Compressing objects:  68% (22/32)   \u001b[K\r",
      "remote: Compressing objects:  71% (23/32)   \u001b[K\r",
      "remote: Compressing objects:  75% (24/32)   \u001b[K\r",
      "remote: Compressing objects:  78% (25/32)   \u001b[K\r",
      "remote: Compressing objects:  81% (26/32)   \u001b[K\r",
      "remote: Compressing objects:  84% (27/32)   \u001b[K\r",
      "remote: Compressing objects:  87% (28/32)   \u001b[K\r",
      "remote: Compressing objects:  90% (29/32)   \u001b[K\r",
      "remote: Compressing objects:  93% (30/32)   \u001b[K\r",
      "remote: Compressing objects:  96% (31/32)   \u001b[K\r",
      "remote: Compressing objects: 100% (32/32)   \u001b[K\r",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "Receiving objects:   0% (1/21065)   \r",
      "Receiving objects:   1% (211/21065)   \r",
      "Receiving objects:   2% (422/21065)   \r",
      "Receiving objects:   3% (632/21065)   \r",
      "Receiving objects:   4% (843/21065)   \r",
      "Receiving objects:   5% (1054/21065)   \r",
      "Receiving objects:   6% (1264/21065)   \r",
      "Receiving objects:   7% (1475/21065)   \r",
      "Receiving objects:   8% (1686/21065)   \r",
      "Receiving objects:   9% (1896/21065)   \r",
      "Receiving objects:  10% (2107/21065)   \r",
      "Receiving objects:  11% (2318/21065)   \r",
      "Receiving objects:  12% (2528/21065)   \r",
      "Receiving objects:  13% (2739/21065)   \r",
      "Receiving objects:  14% (2950/21065)   \r",
      "Receiving objects:  15% (3160/21065)   \r",
      "Receiving objects:  16% (3371/21065)   \r",
      "Receiving objects:  17% (3582/21065)   \r",
      "Receiving objects:  18% (3792/21065)   \r",
      "Receiving objects:  19% (4003/21065)   \r",
      "Receiving objects:  20% (4213/21065)   \r",
      "Receiving objects:  21% (4424/21065)   \r",
      "Receiving objects:  22% (4635/21065)   \r",
      "Receiving objects:  23% (4845/21065)   \r",
      "Receiving objects:  24% (5056/21065)   \r",
      "Receiving objects:  25% (5267/21065)   \r",
      "Receiving objects:  26% (5477/21065)   \r",
      "Receiving objects:  27% (5688/21065)   \r",
      "Receiving objects:  28% (5899/21065)   \r",
      "Receiving objects:  29% (6109/21065)   \r",
      "Receiving objects:  30% (6320/21065)   \r",
      "Receiving objects:  31% (6531/21065)   \r",
      "Receiving objects:  32% (6741/21065)   \r",
      "Receiving objects:  33% (6952/21065)   \r",
      "Receiving objects:  34% (7163/21065)   \r",
      "Receiving objects:  35% (7373/21065)   \r",
      "Receiving objects:  36% (7584/21065)   \r",
      "Receiving objects:  37% (7795/21065)   \r",
      "Receiving objects:  38% (8005/21065)   \r",
      "Receiving objects:  39% (8216/21065)   \r",
      "Receiving objects:  40% (8426/21065)   \r",
      "Receiving objects:  41% (8637/21065)   \r",
      "Receiving objects:  42% (8848/21065)   \r",
      "Receiving objects:  43% (9058/21065)   \r",
      "Receiving objects:  44% (9269/21065)   \r",
      "Receiving objects:  45% (9480/21065)   \r",
      "Receiving objects:  46% (9690/21065)   \r",
      "Receiving objects:  47% (9901/21065)   \r",
      "Receiving objects:  48% (10112/21065)   \r",
      "Receiving objects:  49% (10322/21065)   \r",
      "Receiving objects:  50% (10533/21065)   \r",
      "Receiving objects:  51% (10744/21065)   \r",
      "Receiving objects:  52% (10954/21065)   \r",
      "Receiving objects:  53% (11165/21065)   \r",
      "Receiving objects:  54% (11376/21065)   \r",
      "Receiving objects:  55% (11586/21065)   \r",
      "Receiving objects:  56% (11797/21065)   \r",
      "Receiving objects:  57% (12008/21065)   \r",
      "Receiving objects:  58% (12218/21065)   \r",
      "Receiving objects:  59% (12429/21065)   \r",
      "Receiving objects:  60% (12639/21065)   \r",
      "Receiving objects:  61% (12850/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  62% (13061/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  63% (13271/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  64% (13482/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  65% (13693/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  66% (13903/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  67% (14114/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  68% (14325/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  69% (14535/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  70% (14746/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  71% (14957/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  72% (15167/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  73% (15378/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  74% (15589/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  75% (15799/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  76% (16010/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  77% (16221/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  78% (16431/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  79% (16642/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  80% (16852/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  81% (17063/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  82% (17274/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  83% (17484/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  84% (17695/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  85% (17906/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  86% (18116/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  87% (18327/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  88% (18538/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  89% (18748/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  90% (18959/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  91% (19170/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  92% (19380/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  93% (19591/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  94% (19802/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  95% (20012/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  96% (20223/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  97% (20434/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects:  98% (20644/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "remote: Total 21065 (delta 21), reused 10 (delta 8), pack-reused 21025\u001b[K\n",
      "Receiving objects:  99% (20855/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects: 100% (21065/21065), 15.97 MiB | 31.94 MiB/s   \r",
      "Receiving objects: 100% (21065/21065), 17.71 MiB | 30.58 MiB/s, done.\n",
      "Resolving deltas:   0% (0/14663)   \r",
      "Resolving deltas:   1% (174/14663)   \r",
      "Resolving deltas:   2% (298/14663)   \r",
      "Resolving deltas:   3% (445/14663)   \r",
      "Resolving deltas:   4% (589/14663)   \r",
      "Resolving deltas:   5% (744/14663)   \r",
      "Resolving deltas:   6% (886/14663)   \r",
      "Resolving deltas:   7% (1029/14663)   \r",
      "Resolving deltas:   8% (1175/14663)   \r",
      "Resolving deltas:   9% (1325/14663)   \r",
      "Resolving deltas:  10% (1468/14663)   \r",
      "Resolving deltas:  11% (1622/14663)   \r",
      "Resolving deltas:  12% (1761/14663)   \r",
      "Resolving deltas:  13% (1944/14663)   \r",
      "Resolving deltas:  14% (2063/14663)   \r",
      "Resolving deltas:  15% (2208/14663)   \r",
      "Resolving deltas:  16% (2356/14663)   \r",
      "Resolving deltas:  17% (2529/14663)   \r",
      "Resolving deltas:  18% (2642/14663)   \r",
      "Resolving deltas:  19% (2787/14663)   \r",
      "Resolving deltas:  20% (2938/14663)   \r",
      "Resolving deltas:  21% (3082/14663)   \r",
      "Resolving deltas:  22% (3237/14663)   \r",
      "Resolving deltas:  23% (3414/14663)   \r",
      "Resolving deltas:  24% (3525/14663)   \r",
      "Resolving deltas:  25% (3674/14663)   \r",
      "Resolving deltas:  26% (3815/14663)   \r",
      "Resolving deltas:  27% (3991/14663)   \r",
      "Resolving deltas:  28% (4117/14663)   \r",
      "Resolving deltas:  29% (4261/14663)   \r",
      "Resolving deltas:  30% (4402/14663)   \r",
      "Resolving deltas:  31% (4546/14663)   \r",
      "Resolving deltas:  32% (4694/14663)   \r",
      "Resolving deltas:  33% (4841/14663)   \r",
      "Resolving deltas:  34% (5048/14663)   \r",
      "Resolving deltas:  35% (5133/14663)   \r",
      "Resolving deltas:  36% (5281/14663)   \r",
      "Resolving deltas:  37% (5440/14663)   \r",
      "Resolving deltas:  38% (5588/14663)   \r",
      "Resolving deltas:  39% (5767/14663)   \r",
      "Resolving deltas:  40% (5871/14663)   \r",
      "Resolving deltas:  41% (6021/14663)   \r",
      "Resolving deltas:  42% (6170/14663)   \r",
      "Resolving deltas:  43% (6324/14663)   \r",
      "Resolving deltas:  44% (6492/14663)   \r",
      "Resolving deltas:  45% (6600/14663)   \r",
      "Resolving deltas:  46% (6748/14663)   \r",
      "Resolving deltas:  47% (6941/14663)   \r",
      "Resolving deltas:  48% (7039/14663)   \r",
      "Resolving deltas:  49% (7300/14663)   \r",
      "Resolving deltas:  50% (7355/14663)   \r",
      "Resolving deltas:  51% (7494/14663)   \r",
      "Resolving deltas:  52% (7630/14663)   \r",
      "Resolving deltas:  53% (7782/14663)   \r",
      "Resolving deltas:  54% (7924/14663)   \r",
      "Resolving deltas:  55% (8067/14663)   \r",
      "Resolving deltas:  56% (8277/14663)   \r",
      "Resolving deltas:  57% (8359/14663)   \r",
      "Resolving deltas:  58% (8534/14663)   \r",
      "Resolving deltas:  59% (8671/14663)   \r",
      "Resolving deltas:  60% (8803/14663)   \r",
      "Resolving deltas:  61% (8953/14663)   \r",
      "Resolving deltas:  62% (9095/14663)   \r",
      "Resolving deltas:  63% (9255/14663)   \r",
      "Resolving deltas:  64% (9385/14663)   \r",
      "Resolving deltas:  65% (9536/14663)   \r",
      "Resolving deltas:  66% (9681/14663)   \r",
      "Resolving deltas:  67% (9838/14663)   \r",
      "Resolving deltas:  68% (9971/14663)   \r",
      "Resolving deltas:  69% (10120/14663)   \r",
      "Resolving deltas:  70% (10280/14663)   \r",
      "Resolving deltas:  71% (10412/14663)   \r",
      "Resolving deltas:  72% (10560/14663)   \r",
      "Resolving deltas:  73% (10710/14663)   \r",
      "Resolving deltas:  74% (10946/14663)   \r",
      "Resolving deltas:  75% (10998/14663)   \r",
      "Resolving deltas:  76% (11146/14663)   \r",
      "Resolving deltas:  77% (11337/14663)   \r",
      "Resolving deltas:  78% (11454/14663)   \r",
      "Resolving deltas:  79% (11600/14663)   \r",
      "Resolving deltas:  80% (11731/14663)   \r",
      "Resolving deltas:  81% (11944/14663)   \r",
      "Resolving deltas:  82% (12024/14663)   \r",
      "Resolving deltas:  83% (12179/14663)   \r",
      "Resolving deltas:  84% (12339/14663)   \r",
      "Resolving deltas:  85% (12465/14663)   \r",
      "Resolving deltas:  86% (12611/14663)   \r",
      "Resolving deltas:  87% (12768/14663)   \r",
      "Resolving deltas:  88% (12904/14663)   \r",
      "Resolving deltas:  89% (13054/14663)   \r",
      "Resolving deltas:  90% (13210/14663)   \r",
      "Resolving deltas:  91% (13344/14663)   \r",
      "Resolving deltas:  92% (13492/14663)   \r",
      "Resolving deltas:  93% (13656/14663)   \r",
      "Resolving deltas:  94% (13786/14663)   \r",
      "Resolving deltas:  95% (13931/14663)   \r",
      "Resolving deltas:  96% (14081/14663)   \r",
      "Resolving deltas:  97% (14224/14663)   \r",
      "Resolving deltas:  98% (14370/14663)   \r",
      "Resolving deltas:  99% (14533/14663)   \r",
      "Resolving deltas: 100% (14663/14663)   \r",
      "Resolving deltas: 100% (14663/14663), done.\n",
      "zip_safe flag not set; analyzing archive contents...\n"
     ]
    }
   ],
   "source": [
    "# Remove `> /dev/null` to see the output of commands\n",
    "!rm -fr ~/ParlAI\n",
    "!git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI  > /dev/null\n",
    "!cd ~/ParlAI; python setup.py develop > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A15LLyvRNU65"
   },
   "source": [
    "Most of the scripts that we will use in ParlAI are located in the ~/ParlAI/examples directory.\n",
    "Let's have a first glance at the scripts available, we will come back to them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "8xLsQcefNZr8",
    "outputId": "f1998071-92e3-4e9a-98c1-492c53f3fae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_train.py\t       eval_model.py\t\t remote.py\n",
      "build_dict.py\t       extract_image_feature.py  seq2seq_train_babi.py\n",
      "build_pytorch_data.py  interactive.py\t\t train_model.py\n",
      "display_data.py        profile_train.py\n",
      "display_model.py       README.md\n"
     ]
    }
   ],
   "source": [
    "!ls ~/ParlAI/examples/   ## all the scripts or files that we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8BCBD6vOY6V"
   },
   "source": [
    "## 1. Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "stay4iHqOcVq"
   },
   "source": [
    "**First** we need to download the data, we will use the `build_dict.py` as a dummy task to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "9c2i-fGaOQOv",
    "outputId": "6cb1a518-a74a-48fd-9c87-16d51330aa86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading babi.tar.gz: 100% 19.2M/19.2M [00:01<00:00, 9.81MB/s]\n",
      "Building dictionary: 100% 900/900 [00:00<00:00, 17.9kex/s]\n",
      "1 Mary moved to the bathroom.\n",
      "2 John went to the hallway.\n",
      "3 Where is Mary? \tbathroom\n",
      "4 Daniel went back to the hallway.\n",
      "5 Sandra moved to the garden.\n",
      "6 Where is Daniel? \thallway\n",
      "7 John moved to the office.\n",
      "8 Sandra journeyed to the bathroom.\n",
      "9 Where is Daniel? \thallway\n",
      "10 Mary moved to the hallway.\n",
      "11 Daniel travelled to the office.\n",
      "12 Where is Daniel? \toffice\n",
      "13 John went back to the garden.\n",
      "14 John moved to the bedroom.\n",
      "15 Where is Sandra? \tbathroom\n",
      "1 Mary went to the bedroom.\n",
      "2 John journeyed to the bathroom.\n",
      "3 Where is John? \tbathroom\n",
      "4 Sandra journeyed to the hallway.\n",
      "5 John journeyed to the garden.\n",
      "6 Where is Mary? \tbedroom\n",
      "7 John journeyed to the bathroom.\n",
      "8 Sandra journeyed to the garden.\n",
      "9 Where is John? \tbathroom\n",
      "10 Sandra went back to the bedroom.\n",
      "11 Daniel travelled to the bathroom.\n",
      "12 Where is John? \tbathroom\n",
      "13 John went to the office.\n",
      "14 Mary moved to the office.\n",
      "15 Where is Sandra? \tbedroom\n"
     ]
    }
   ],
   "source": [
    "# Download the data silently\n",
    "!python ~/ParlAI/examples/build_dict.py --task babi:task1k:1 --dict-file /tmp/babi1.dict > /dev/null\n",
    "# Print a few examples\n",
    "!head -n 30 ~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apxdE1h0OoKs"
   },
   "source": [
    "The bAbI tasks were downloaded in `~/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-nosf/`\n",
    "\n",
    "In bAbI the data is organised as follows:\n",
    "- **Dialog turn**: A dialog turn is a single utterance / statement. Each line in the file corresponds to one dialog turn.   \n",
    "  Example: *\"John went to the office.\"*\n",
    "- **Sample (question)**: Every few dialog turns, a question can be asked that the model has to answer, this consitute a sample.  The question is followed by its ground truth answer, separated by a tab.\n",
    "  Example: *\"Where is John? `<tab>` bathroom\"*\n",
    "- **Episode**: a sequence of ordered coherent dialog turns that are related to each other form an episode. Each new episode is independant of the others. Each line starts with the dialog turn number in the current episode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-XeHnQIO_3c"
   },
   "source": [
    "## 3. Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgge2s_2O9z0"
   },
   "source": [
    "We now have a clearer idea of the data distribution and the metrics that we can use.  \n",
    "The next step is to start solving the tasks with a simple baseline. This will allow us to compare more elaborate models agains this baseline.  \n",
    "Here are a few classical baselines:\n",
    "- **Random model**: The model answers randomly among the set of possible answers for each question\n",
    "-  **Majority class**: The model always answers with the most frequent answer in the training set (majority class)\n",
    "\n",
    "We are going to reimplement these own baselines.  \n",
    "Implementing a new model in ParlAI is detailed in the [tutorial](http://parl.ai/static/docs/seq2seq_tutorial.html) but for our simple baselines, we will only need to inherit the [Agent](https://github.com/facebookresearch/ParlAI/blob/6d246842d3f4e941dd3806f3d9fa62f607d48f59/parlai/core/agents.py#L50) class and override the `act()` method.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Note: the `%%writefile` magic command in jupyter writes the content of the cell to a file at the given path.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJJQF0MwOwW9"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/ParlAI/parlai/agents/baseline/  ## create a folder to store our baseline files\n",
    "!touch ~/ParlAI/parlai/agents/baseline/random.py ## create the random baseline file\n",
    "!touch ~/ParlAI/parlai/agents/baseline/majorityclass.py ## create the majority baseline file\n",
    "\n",
    "!mkdir -p ~/ParlAI/parlai/agents/rnn_model/    ## create a folder to store rnn model\n",
    "!touch ~/ParlAI/parlai/agents/rnn_model/rnn_model.py ## create rnn model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSanafhW5kFA"
   },
   "source": [
    "In the next cell we are going to create the random baseline which consist to pick an answer randomly from all the candidates answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aT1k7cmVQvO7",
    "outputId": "ac461115-24b5-4237-cec9-82f733bc20dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/baseline/random.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/ParlAI/parlai/agents/baseline/random.py\n",
    "import random\n",
    "\n",
    "from parlai.core.torch_agent import Agent   ## import the Agent model where our baseline will inherit\n",
    "\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "  \n",
    "    def act(self):\n",
    "        if 'label_candidates' not in self.observation:   ## check if the attribute label_candidates is part of oservation if not we will not do anything\n",
    "            return\n",
    "        candidates = list(self.observation['label_candidates'])  ## the cadidates answer\n",
    "        reply = {'text': candidates[random.randrange(len(candidates))]} ## choose randomly one of the candidate answer\n",
    "        return reply  ## and return the choosen answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfGjpkISRCRC"
   },
   "outputs": [],
   "source": [
    "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/random | grep accuracy     ##run the random baseline and check the accuracy of that baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1745
    },
    "colab_type": "code",
    "id": "k7PfK0lJRHIT",
    "outputId": "3bb556aa-5502-43ed-ae18-9f84b5d4c1d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: baseline/random ]\n",
      "[  model_file: None ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: bathroom]\n",
      "   kitchen\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: bathroom]\n",
      "   garden\n",
      "~~\n",
      "[babi:task10k:1]: John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: bathroom]\n",
      "   garden\n",
      "~~\n",
      "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   kitchen\n",
      "~~\n",
      "[babi:task10k:1]: John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: hallway]\n",
      "   bathroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: garden]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   bedroom\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   kitchen\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[label_candidates: garden|bedroom|bathroom|office|hallway|...and 1 more]\n",
      "[eval_labels: office]\n",
      "   garden\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/random -n 10   ## look at the first 10 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "485qcDw_RNrx",
    "outputId": "fb879f00-b68c-4a9a-fa3c-7adc92b3611b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/baseline/majorityclass.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/ParlAI/parlai/agents/baseline/majorityclass.py\n",
    "import random\n",
    "\n",
    "from parlai.core.torch_agent import Agent\n",
    "\n",
    "\n",
    "class MajorityclassAgent(Agent):\n",
    "  \n",
    "    def act(self):\n",
    "        # From the previous answers:\n",
    "        # Possible answers: Counter({'bathroom': 1564, 'hallway': 1517, 'garden': 1508, 'bedroom': 1473, 'kitchen': 1471, 'office': 1467}) (6)\n",
    "        # So the most common answer is 'bathroom'\n",
    "        reply = {'text': 'bathroom'}\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "act6OmTGRZEm",
    "outputId": "d3313ec3-4ce8-4f95-db67-8ee278734882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exs': 1000, 'accuracy': 0.169, 'f1': 0.169, 'bleu': 1.69e-10}\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/eval_model.py -t babi:task10k:1 -m baseline/majorityclass | grep accuracy  ##run the majority baseline and check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1745
    },
    "colab_type": "code",
    "id": "klWsVvXIRaub",
    "outputId": "13307f9a-5a78-4407-a2fe-0843180aa54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_ignore_fields:  ]\n",
      "[  num_examples: 10 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: None ]\n",
      "[  init_model: None ]\n",
      "[  model: baseline/majorityclass ]\n",
      "[  model_file: None ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[babi:task10k:1]: Sandra travelled to the office.\n",
      "Sandra went to the bathroom.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the bedroom.\n",
      "Daniel moved to the hallway.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the garden.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: bathroom]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Daniel journeyed to the bedroom.\n",
      "Daniel travelled to the hallway.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went to the bedroom.\n",
      "John travelled to the office.\n",
      "Where is Daniel?\n",
      "[eval_labels: hallway]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n",
      "[babi:task10k:1]: Sandra went back to the bathroom.\n",
      "Mary moved to the garden.\n",
      "Where is Mary?\n",
      "[eval_labels: garden]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went back to the hallway.\n",
      "Sandra went to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: John went back to the hallway.\n",
      "John travelled to the office.\n",
      "Where is Sandra?\n",
      "[eval_labels: office]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Sandra journeyed to the hallway.\n",
      "Daniel moved to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "~~\n",
      "[babi:task10k:1]: Mary went to the office.\n",
      "Sandra went to the office.\n",
      "Where is John?\n",
      "[eval_labels: office]\n",
      "[label_candidates: bedroom|kitchen|office|garden|hallway|...and 1 more]\n",
      "   bathroom\n",
      "- - - - - - - - - - - - - - - - - - - - -\n",
      "~~\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/display_model.py -t babi:task10k:1 -m baseline/majorityclass -n 10 ## Print the first top 10 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzryysXmISgd"
   },
   "source": [
    "### Intilaize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "I-0gkt_TRcDl",
    "outputId": "52f5e46c-b30d-4ab9-8122-405a49c974fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/ParlAI/parlai/agents/rnn_model/rnn_model.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile ~/ParlAI/parlai/agents/rnn_model/rnn_model.py\n",
    "##import parlai and pytorch\n",
    "from parlai.core.torch_agent import TorchAgent, Output\n",
    "from parlai.core.utils import padded_3d\n",
    "from parlai.core.logs import TensorboardLogger\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "losses = []\n",
    "##define the model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size ## input of the model: number of characters in our dictionary\n",
    "        self.hidden_size = hidden_size ## number of neurons per layers\n",
    "        self.output_size = output_size ## output size of the model: number of charaters in the output\n",
    "        self.n_layers = n_layers  ## number of layers of the model\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)## map a characters to a fixed size of vector (for characters embeddings)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers) ## define the GRU part:it takes asinput the embedding vectors from the encoder\n",
    "        self.decoder = nn.Linear(hidden_size, output_size) ## decode the vectors output of the GRU to characters \n",
    "    \n",
    "    def forward(self, input, hidden): \n",
    "        input = self.encoder(input)\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        output = self.decoder(output)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=32):\n",
    "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)) ##initialize all the hidden state to zero\n",
    "\n",
    "class RnnModelAgent(TorchAgent):\n",
    "    \n",
    "    @staticmethod    \n",
    "    def add_cmdline_args(argparser):  ## to allow to run the model on command line\n",
    "        agent = argparser.add_argument_group(\"Simple RNN Arguments\")\n",
    "        \n",
    "        agent.add_argument(\"-hs\", \"--hidden-size\", type=int, default=128, help=\"Size of the hidden layer(s) of the RNN\")\n",
    "        agent.add_argument(\"-nl\", \"--num-layers\", type=int, default=1, help=\"Number of layers of the RNN\")\n",
    "        \n",
    "        TorchAgent.add_cmdline_args(argparser)\n",
    "        RnnModelAgent.dictionary_class().add_cmdline_args(argparser)\n",
    "        \n",
    "        return agent\n",
    "    \n",
    "    def __init__(self, opt, shared=None):\n",
    "        \n",
    "        super().__init__(opt, shared)\n",
    "        \n",
    "        if opt['tensorboard_log'] is True:\n",
    "            self.writer = TensorboardLogger(opt)\n",
    "        \n",
    "        self.dictionnary_size = 26      ## we suppose that our dictionary contain the 26 characters of the alphabet\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "        self.use_cuda = True if torch.cuda.is_available() else False\n",
    "        self.embedding_dim = opt[\"hidden_size\"]  ##the dimension of our embemding vector after transforming the character to a vector\n",
    "        self.n_layers = opt[\"num_layers\"]  ## number of layer of the network\n",
    "        self.batch_size = opt[\"batchsize\"]  ## number of examples that we are going to pass as an input at the same time to the network\n",
    "        self.criterion = nn.NLLLoss()  ### define the loss of our network\n",
    "        self.decoder = RNN(self.dictionnary_size, self.embedding_dim, self.dictionnary_size, self.n_layers) ## This RNN refer to the previous RNN class\n",
    "        self.decoder.to(self.device)\n",
    "        self.model = self.decoder\n",
    "        \n",
    "        def weight_init(m):   ##initialisation of the weights of our model using xavier\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "        \n",
    "        self.decoder.apply(weight_init)    #apply the initialized weight to the model\n",
    "        self.optimizer = optim.Adam(self.decoder.parameters()) ## use Adam optimizer for the model to perform backpropagation(weight updating)\n",
    "        self.scheduler = None\n",
    "        self.warmup_scheduler = None\n",
    "        self.batch_iter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    def vectorize(self, *args, **kwargs):    ## this function is called by parlai to preprocess text\n",
    "        \"\"\"Override options in vectorize from parent.\"\"\"\n",
    "        kwargs['add_start'] = False       ##to avoid starting at  the start of sentence\n",
    "        kwargs['add_end'] = False\n",
    "        return super().vectorize(*args, **kwargs)\n",
    "      \n",
    "    ## the next function allows us to train the model we defined so far\n",
    "    def train_step(self, batch):  \n",
    "        \n",
    "        self.decoder.train()\n",
    "        \n",
    "        contexts, answers = batch.text_vec, batch.label_vec # extract the different contexts and answers from the batch input\n",
    "        \n",
    "        self.optimizer.zero_grad() ## Since the backward() function accumulates gradients, and we don’t want to mix up gradients between minibatches,\n",
    "                                   ## we have to zero them out at the start of a new minibatch\n",
    "\n",
    "        loss = 0\n",
    "        hidden = self.decoder.init_hidden(self.batch_size).to(self.device) ## initialize the hidden state to zero by calling the previous init_hidden function\n",
    "\n",
    "        output, hidden = self.decoder(contexts.t(), hidden) \n",
    "        output = output.transpose(0,1)[:,-1,:]              ## to get sequence leng, batch, dimension\n",
    "        loss += self.criterion(output.squeeze(1), answers.squeeze(1)) ## compute the loss by comparing the output of our model and the true answer\n",
    "            \n",
    "        \n",
    "        pred = output.argmax(dim=1)  ## get the predicted answer of our model: it appear at the index which has the bigger value of probbility\n",
    "        \n",
    "        losses.append(loss.item())  ## accumulate the loss\n",
    "         \n",
    "        loss.backward() ## backpropagate the loss to update the parameters of the model\n",
    "        self.optimizer.step()  #specify that we finish with one step optimization \n",
    "        \n",
    "        self.batch_iter+= 1 ## next iteration\n",
    "        \n",
    "        return Output(self.dict.vec2txt(pred).split(\" \")) # convert the vector output to text and return it\n",
    "    \n",
    "    def eval_step(self, batch): #after training the model we need to evaluate it in order to measure its performance\n",
    "        contexts = batch.text_vec ## in the evaluation case we don't have acces to an asnwer, the only thing we have is the question\n",
    "        \n",
    "        hidden = self.decoder.init_hidden(contexts.shape[0]).to(self.device) ## as usually we initialize the hidden state\n",
    "\n",
    "        output, hidden = self.decoder(contexts.t(), hidden)  # compute the output of the model on unseen data\n",
    "        output = output.transpose(0,1)[:,-1,:]    ## to get sequence leng, batch, dimension\n",
    "            \n",
    "        \n",
    "        pred = output.argmax(dim=1)  # get the predicte\n",
    "        \n",
    "        return Output(self.dict.vec2txt(pred).split(\" \"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "PJjg2kKiVn-Q",
    "outputId": "2dd10c60-83df-481c-843c-c492c1157375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babi1.dict\t   model2.opt\t      my_rnn_model.dict.opt\n",
      "babi1.dict.opt\t   model2.test\t      my_rnn_model.opt\n",
      "model2\t\t   model2.trainstats  my_rnn_model.test\n",
      "model2.best_valid  model2.valid       my_rnn_model.trainstats\n",
      "model2.dict\t   my_rnn_model       my_rnn_model.valid\n",
      "model2.dict.opt    my_rnn_model.dict\n"
     ]
    }
   ],
   "source": [
    "#!rm -fr /tmp/*\n",
    "!ls /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3005
    },
    "colab_type": "code",
    "id": "IiNtn4-Ht8s0",
    "outputId": "6e5fe9f5-b38c-4d5d-e9b7-744b8a079528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 32 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: rnn_model ]\n",
      "[  model_file: /tmp/my_rnn_model ]\n",
      "[ Training Loop Arguments: ] \n",
      "[  dict_build_first: True ]\n",
      "[  display_examples: False ]\n",
      "[  eval_batchsize: None ]\n",
      "[  evaltask: None ]\n",
      "[  load_from_checkpoint: False ]\n",
      "[  max_train_time: -1 ]\n",
      "[  num_epochs: 10.0 ]\n",
      "[  save_after_valid: False ]\n",
      "[  save_every_n_secs: -1 ]\n",
      "[  short_final_eval: False ]\n",
      "[  validation_cutoff: 1.0 ]\n",
      "[  validation_every_n_epochs: -1 ]\n",
      "[  validation_every_n_secs: -1 ]\n",
      "[  validation_max_exs: -1 ]\n",
      "[  validation_metric: accuracy ]\n",
      "[  validation_metric_mode: None ]\n",
      "[  validation_patience: 10 ]\n",
      "[  validation_share_agent: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_comment:  ]\n",
      "[  tensorboard_log: False ]\n",
      "[  tensorboard_metrics: None ]\n",
      "[  tensorboard_tag: None ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Dictionary Loop Arguments: ] \n",
      "[  dict_include_test: False ]\n",
      "[  dict_include_valid: False ]\n",
      "[  dict_maxexs: -1 ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Simple RNN Arguments: ] \n",
      "[  hidden_size: 128 ]\n",
      "[  num_layers: 1 ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "[ building dictionary first... ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[ running dictionary over data.. ]\n",
      "Building dictionary:   0% 0.00/9.00k [00:00<?, ?ex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "Building dictionary:  85% 7.64k/9.00k [00:00<00:00, 19.1kex/s][loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "Building dictionary: 100% 9.00k/9.00k [00:00<00:00, 19.1kex/s]\n",
      "Dictionary: saving dictionary to /tmp/my_rnn_model.dict\n",
      "[ dictionary built with 26 tokens in 0s ]\n",
      "[ no model with opt yet at: /tmp/my_rnn_model(.opt) ]\n",
      "Dictionary: loading dictionary from /tmp/my_rnn_model.dict\n",
      "[ num words =  26 ]\n",
      "[ Using CUDA ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_train.txt]\n",
      "[ training... ]\n",
      "[ time:2.0s total_exs:2848 epochs:0.32 time_left:62.0s ] {'exs': 2848, 'accuracy': 0.3164, 'f1': 0.3164, 'bleu': 3.164e-10, 'num_updates': 0}\n",
      "[ time:4.0s total_exs:5696 epochs:0.63 time_left:60.0s ] {'exs': 2848, 'accuracy': 0.5014, 'f1': 0.5014, 'bleu': 5.014e-10, 'num_updates': 0}\n",
      "[ time:6.0s total_exs:8544 epochs:0.95 time_left:58.0s ] {'exs': 2848, 'accuracy': 0.4737, 'f1': 0.4737, 'bleu': 4.737e-10, 'num_updates': 0}\n",
      "[ time:8.0s total_exs:11360 epochs:1.26 time_left:56.0s ] {'exs': 2816, 'accuracy': 0.4773, 'f1': 0.4773, 'bleu': 4.773e-10, 'num_updates': 0}\n",
      "[ time:10.0s total_exs:14208 epochs:1.58 time_left:54.0s ] {'exs': 2848, 'accuracy': 0.4673, 'f1': 0.4673, 'bleu': 4.673e-10, 'num_updates': 0}\n",
      "[ time:12.0s total_exs:17056 epochs:1.9 time_left:52.0s ] {'exs': 2848, 'accuracy': 0.4782, 'f1': 0.4782, 'bleu': 4.782e-10, 'num_updates': 0}\n",
      "[ time:14.0s total_exs:19904 epochs:2.21 time_left:50.0s ] {'exs': 2848, 'accuracy': 0.4638, 'f1': 0.4638, 'bleu': 4.638e-10, 'num_updates': 0}\n",
      "[ time:16.0s total_exs:22720 epochs:2.52 time_left:48.0s ] {'exs': 2816, 'accuracy': 0.4648, 'f1': 0.4648, 'bleu': 4.648e-10, 'num_updates': 0}\n",
      "[ time:18.0s total_exs:25568 epochs:2.84 time_left:46.0s ] {'exs': 2848, 'accuracy': 0.4477, 'f1': 0.4477, 'bleu': 4.477e-10, 'num_updates': 0}\n",
      "[ time:20.0s total_exs:28416 epochs:3.16 time_left:44.0s ] {'exs': 2848, 'accuracy': 0.461, 'f1': 0.461, 'bleu': 4.61e-10, 'num_updates': 0}\n",
      "[ time:22.0s total_exs:31264 epochs:3.47 time_left:42.0s ] {'exs': 2848, 'accuracy': 0.4572, 'f1': 0.4572, 'bleu': 4.572e-10, 'num_updates': 0}\n",
      "[ time:24.0s total_exs:34112 epochs:3.79 time_left:40.0s ] {'exs': 2848, 'accuracy': 0.4558, 'f1': 0.4558, 'bleu': 4.558e-10, 'num_updates': 0}\n",
      "[ time:26.0s total_exs:36960 epochs:4.11 time_left:38.0s ] {'exs': 2848, 'accuracy': 0.4589, 'f1': 0.4589, 'bleu': 4.589e-10, 'num_updates': 0}\n",
      "[ time:28.0s total_exs:39808 epochs:4.42 time_left:36.0s ] {'exs': 2848, 'accuracy': 0.4596, 'f1': 0.4596, 'bleu': 4.596e-10, 'num_updates': 0}\n",
      "[ time:30.0s total_exs:42656 epochs:4.74 time_left:34.0s ] {'exs': 2848, 'accuracy': 0.4954, 'f1': 0.4954, 'bleu': 4.954e-10, 'num_updates': 0}\n",
      "[ time:32.0s total_exs:45504 epochs:5.06 time_left:32.0s ] {'exs': 2848, 'accuracy': 0.4635, 'f1': 0.4635, 'bleu': 4.635e-10, 'num_updates': 0}\n",
      "[ time:34.0s total_exs:48320 epochs:5.37 time_left:30.0s ] {'exs': 2816, 'accuracy': 0.4538, 'f1': 0.4538, 'bleu': 4.538e-10, 'num_updates': 0}\n",
      "[ time:36.0s total_exs:51168 epochs:5.69 time_left:28.0s ] {'exs': 2848, 'accuracy': 0.4705, 'f1': 0.4705, 'bleu': 4.705e-10, 'num_updates': 0}\n",
      "[ time:38.0s total_exs:54016 epochs:6.0 time_left:26.0s ] {'exs': 2848, 'accuracy': 0.4695, 'f1': 0.4695, 'bleu': 4.695e-10, 'num_updates': 0}\n",
      "[ time:40.0s total_exs:56864 epochs:6.32 time_left:24.0s ] {'exs': 2848, 'accuracy': 0.474, 'f1': 0.474, 'bleu': 4.74e-10, 'num_updates': 0}\n",
      "[ time:42.0s total_exs:59680 epochs:6.63 time_left:22.0s ] {'exs': 2816, 'accuracy': 0.473, 'f1': 0.473, 'bleu': 4.73e-10, 'num_updates': 0}\n",
      "[ time:44.0s total_exs:62560 epochs:6.95 time_left:20.0s ] {'exs': 2880, 'accuracy': 0.4931, 'f1': 0.4931, 'bleu': 4.931e-10, 'num_updates': 0}\n",
      "[ time:46.0s total_exs:65440 epochs:7.27 time_left:18.0s ] {'exs': 2880, 'accuracy': 0.4917, 'f1': 0.4917, 'bleu': 4.917e-10, 'num_updates': 0}\n",
      "[ time:48.0s total_exs:68288 epochs:7.59 time_left:16.0s ] {'exs': 2848, 'accuracy': 0.4954, 'f1': 0.4954, 'bleu': 4.954e-10, 'num_updates': 0}\n",
      "[ time:50.0s total_exs:71136 epochs:7.9 time_left:14.0s ] {'exs': 2848, 'accuracy': 0.5063, 'f1': 0.5063, 'bleu': 5.063e-10, 'num_updates': 0}\n",
      "[ time:52.0s total_exs:73984 epochs:8.22 time_left:12.0s ] {'exs': 2848, 'accuracy': 0.5109, 'f1': 0.5109, 'bleu': 5.109e-10, 'num_updates': 0}\n",
      "[ time:54.0s total_exs:76800 epochs:8.53 time_left:10.0s ] {'exs': 2816, 'accuracy': 0.4989, 'f1': 0.4989, 'bleu': 4.989e-10, 'num_updates': 0}\n",
      "[ time:56.0s total_exs:79648 epochs:8.85 time_left:8.0s ] {'exs': 2848, 'accuracy': 0.5488, 'f1': 0.5488, 'bleu': 5.488e-10, 'num_updates': 0}\n",
      "[ time:58.0s total_exs:82496 epochs:9.17 time_left:6.0s ] {'exs': 2848, 'accuracy': 0.5421, 'f1': 0.5421, 'bleu': 5.421e-10, 'num_updates': 0}\n",
      "[ time:60.0s total_exs:85344 epochs:9.48 time_left:4.0s ] {'exs': 2848, 'accuracy': 0.565, 'f1': 0.565, 'bleu': 5.65e-10, 'num_updates': 0}\n",
      "[ time:62.0s total_exs:88160 epochs:9.8 time_left:2.0s ] {'exs': 2816, 'accuracy': 0.5781, 'f1': 0.5781, 'bleu': 5.781e-10, 'num_updates': 0}\n",
      "[ time:63.0s total_exs:90016 epochs:10.0 time_left:0s ] {'exs': 1856, 'accuracy': 0.6002, 'f1': 0.6002, 'bleu': 6.002e-10, 'num_updates': 0}\n",
      "[ num_epochs completed:10.0 time elapsed:63.68912720680237s ]\n",
      "Dictionary: saving dictionary to /tmp/my_rnn_model.dict\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[ running eval: valid ]\n",
      "valid:{'exs': 1000, 'accuracy': 0.554, 'f1': 0.554, 'bleu': 5.54e-10, 'num_updates': 0}\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_test.txt]\n",
      "[ running eval: test ]\n",
      "test:{'exs': 1000, 'accuracy': 0.519, 'f1': 0.519, 'bleu': 5.19e-10, 'num_updates': 0}\n"
     ]
    }
   ],
   "source": [
    "!rm -fr /tmp/my_rnn_model*\n",
    "!python ~/ParlAI/examples/train_model.py -t babi:task10k:1 -bs 32 -eps 10 -m rnn_model -mf /tmp/my_rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1817
    },
    "colab_type": "code",
    "id": "c4NmxeDbYDay",
    "outputId": "1005909c-de75-41cd-9fa7-d8ff51c52eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ warning: overriding opt['datatype'] to valid (previously: train )]\n",
      "Dictionary: loading dictionary from /tmp/my_rnn_model.dict\n",
      "[ num words =  26 ]\n",
      "[ Using CUDA ]\n",
      "[creating task(s): babi:task10k:1]\n",
      "[loading fbdialog data:/root/ParlAI/data/bAbI/tasks_1-20_v1-2/en-valid-10k-nosf/qa1_valid.txt]\n",
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  log_every_n_secs: 2 ]\n",
      "[  metrics: all ]\n",
      "[  num_examples: -1 ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 32 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: valid ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: rnn_model ]\n",
      "[  model_file: /tmp/my_rnn_model ]\n",
      "[ PytorchData Arguments: ] \n",
      "[  batch_length_range: 5 ]\n",
      "[  batch_sort_cache_type: pop ]\n",
      "[  batch_sort_field: text ]\n",
      "[  numworkers: 4 ]\n",
      "[  pytorch_context_length: -1 ]\n",
      "[  pytorch_datapath: None ]\n",
      "[  pytorch_include_labels: True ]\n",
      "[  pytorch_preprocess: False ]\n",
      "[  pytorch_teacher_batch_sort: False ]\n",
      "[  pytorch_teacher_dataset: None ]\n",
      "[  pytorch_teacher_task: None ]\n",
      "[  shuffle: False ]\n",
      "[ Tensorboard Arguments: ] \n",
      "[  tensorboard_comment:  ]\n",
      "[  tensorboard_log: False ]\n",
      "[  tensorboard_metrics: None ]\n",
      "[  tensorboard_tag: None ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Simple RNN Arguments: ] \n",
      "[  hidden_size: 128 ]\n",
      "[  num_layers: 1 ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: [0.9, 0.999] ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: [0.7] ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: /tmp/my_rnn_model.dict ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "2s elapsed: {'exs': 744, '%done': '74.40%', 'time_left': '0s', 'accuracy': 0, 'f1': 0, 'bleu': 0, 'num_updates': 0}\n",
      "EPOCH DONE\n",
      "finished evaluating task babi:task10k:1 using datatype valid\n",
      "{'exs': 1000, 'accuracy': 0, 'f1': 0, 'bleu': 0, 'num_updates': 0}\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/eval_model.py -mf /tmp/my_rnn_model -t babi:task10k:1 -dt valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2987
    },
    "colab_type": "code",
    "id": "Ian7bIWxT_Cm",
    "outputId": "9f8c7dd7-5c56-434e-8c3c-9fd716a478d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  display_ignore_fields: label_candidates,text_candidates ]\n",
      "[  display_prettify: False ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 1 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: None ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: None ]\n",
      "[  model_file: /tmp/my_rnn_model ]\n",
      "[ Local Human Arguments: ] \n",
      "[  local_human_candidates_file: None ]\n",
      "[  single_turn: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Simple RNN Arguments: ] \n",
      "[  hidden_size: 128 ]\n",
      "[  num_layers: 1 ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: (0.9, 0.999) ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: (0.7,) ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: None ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "Dictionary: loading dictionary from /tmp/my_rnn_model.dict\n",
      "[ num words =  26 ]\n",
      "[ Using CUDA ]\n",
      "[creating task(s): parlai.agents.local_human.local_human:LocalHumanAgent]\n",
      "[ optional arguments: ] \n",
      "[  display_examples: False ]\n",
      "[  display_ignore_fields: label_candidates,text_candidates ]\n",
      "[  display_prettify: False ]\n",
      "[ Main ParlAI Arguments: ] \n",
      "[  batchsize: 32 ]\n",
      "[  datapath: /root/ParlAI/data ]\n",
      "[  datatype: train ]\n",
      "[  download_path: /root/ParlAI/downloads ]\n",
      "[  hide_labels: False ]\n",
      "[  image_mode: raw ]\n",
      "[  multitask_weights: [1] ]\n",
      "[  numthreads: 1 ]\n",
      "[  show_advanced_args: False ]\n",
      "[  task: babi:task10k:1 ]\n",
      "[ ParlAI Model Arguments: ] \n",
      "[  dict_class: parlai.core.dict:DictionaryAgent ]\n",
      "[  init_model: None ]\n",
      "[  model: rnn_model ]\n",
      "[  model_file: /tmp/my_rnn_model ]\n",
      "[ Local Human Arguments: ] \n",
      "[  local_human_candidates_file: None ]\n",
      "[  single_turn: False ]\n",
      "[ ParlAI Image Preprocessing Arguments: ] \n",
      "[  image_cropsize: 224 ]\n",
      "[  image_size: 256 ]\n",
      "[ Simple RNN Arguments: ] \n",
      "[  hidden_size: 128 ]\n",
      "[  num_layers: 1 ]\n",
      "[ TorchAgent Arguments: ] \n",
      "[  add_p1_after_newln: False ]\n",
      "[  betas: [0.9, 0.999] ]\n",
      "[  delimiter: \n",
      " ]\n",
      "[  embedding_projection: random ]\n",
      "[  embedding_type: random ]\n",
      "[  fp16: False ]\n",
      "[  gpu: -1 ]\n",
      "[  gradient_clip: 0.1 ]\n",
      "[  history_size: -1 ]\n",
      "[  label_truncate: None ]\n",
      "[  learningrate: 1 ]\n",
      "[  lr_scheduler: reduceonplateau ]\n",
      "[  lr_scheduler_decay: 0.5 ]\n",
      "[  lr_scheduler_patience: 3 ]\n",
      "[  momentum: 0 ]\n",
      "[  nesterov: True ]\n",
      "[  no_cuda: False ]\n",
      "[  nus: [0.7] ]\n",
      "[  optimizer: sgd ]\n",
      "[  person_tokens: False ]\n",
      "[  rank_candidates: False ]\n",
      "[  split_lines: False ]\n",
      "[  text_truncate: None ]\n",
      "[  truncate: -1 ]\n",
      "[  update_freq: -1 ]\n",
      "[  use_reply: label ]\n",
      "[  warmup_rate: 0.0001 ]\n",
      "[  warmup_updates: -1 ]\n",
      "[ Dictionary Arguments: ] \n",
      "[  bpe_debug: False ]\n",
      "[  dict_endtoken: __end__ ]\n",
      "[  dict_file: /tmp/my_rnn_model.dict ]\n",
      "[  dict_initpath: None ]\n",
      "[  dict_language: english ]\n",
      "[  dict_lower: False ]\n",
      "[  dict_max_ngram_size: -1 ]\n",
      "[  dict_maxtokens: -1 ]\n",
      "[  dict_minfreq: 0 ]\n",
      "[  dict_nulltoken: __null__ ]\n",
      "[  dict_starttoken: __start__ ]\n",
      "[  dict_textfields: text,labels ]\n",
      "[  dict_tokenizer: re ]\n",
      "[  dict_unktoken: __unk__ ]\n",
      "Enter Your Message: Bob is Blue.\\nWhat is Bob?\n",
      "[TorchAgent]: __null__\n",
      "Enter Your Message: Traceback (most recent call last):\n",
      "  File \"/root/ParlAI/examples/interactive.py\", line 18, in <module>\n",
      "    interactive(opt, print_parser=parser)\n",
      "  File \"/root/ParlAI/parlai/scripts/interactive.py\", line 64, in interactive\n",
      "    world.parley()\n",
      "  File \"/root/ParlAI/parlai/core/worlds.py\", line 257, in parley\n",
      "    acts[0] = agents[0].act()\n",
      "  File \"/root/ParlAI/parlai/agents/local_human/local_human.py\", line 39, in act\n",
      "    reply_text = input(\"Enter Your Message: \")\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python ~/ParlAI/examples/interactive.py -mf /tmp/my_rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tvRZB5bC4-a"
   },
   "source": [
    "# Part 2 (Optional) : Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "OtgM5hUPYffX",
    "outputId": "07164737-8bea-4870-9cc0-7552bfc7470f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-13 11:54:24--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
      "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
      "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4573338 (4.4M) [text/plain]\n",
      "Saving to: ‘shakespeare_input.txt’\n",
      "\n",
      "shakespeare_input.t 100%[===================>]   4.36M  7.59MB/s    in 0.6s    \n",
      "\n",
      "2019-04-13 11:54:25 (7.59 MB/s) - ‘shakespeare_input.txt’ saved [4573338/4573338]\n",
      "\n",
      "sample_data  shakespeare_input.txt\n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "QadAOrUVcqb3",
    "outputId": "0c062a04-82d6-4797-88b5-c84c379376ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/39/53096f9217b057cb049fe872b7fc7ce799a1a89b76cf917d9639e7a558b5/Unidecode-1.0.23-py2.py3-none-any.whl (237kB)\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 6.4MB/s \n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.0.23\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "zOd_X6HEY9HC",
    "outputId": "141d67c5-4a86-4ec7-ee00-7affb09b031c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4573338\n",
      "n_characters :  100\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('shakespeare_input.txt').read())\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n",
    "print(\"n_characters : \", n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "mRdLhL08ZYCI",
    "outputId": "3910d650-05e7-4a58-fca8-c4e48acf9040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ysician;\n",
      "His antidotes are poison, and he slays\n",
      "Moe than you rob: take wealth and lives together;\n",
      "Do villany, do, since you protest to do't,\n",
      "Like workmen. I'll example you with thievery.\n",
      "The sun's a th\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlUsTGXvZZhm"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fXoDhfcUdZof",
    "outputId": "1a984ac4-3a74-48d0-d911-fe32200b7ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm2HMLCdddmL"
   },
   "outputs": [],
   "source": [
    "def random_training_set():    \n",
    "    chunk = random_chunk()\n",
    "    inp = char_tensor(chunk[:-1])\n",
    "    target = char_tensor(chunk[1:])\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_P2iFx7diGe"
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden().to(device)\n",
    "    prime_input = char_tensor(prime_str).to(device)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1].to(device)    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char).to(device)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mf3Z7Nrbdl_u"
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Ehw695MdohO"
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden().to(device)\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].reshape(-1))\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data / chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1198
    },
    "colab_type": "code",
    "id": "VloBXyH0d54s",
    "outputId": "f7ddbf63-ed73-4b86-da7f-1aaa40da18f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 34s (100 1%) 2.4968]\n",
      "Whean\n",
      "Beusn,\n",
      "Thum heroy nerekal bann thoun mereanswivhalet can coursde thinnd sit fhabs won! me bnin!  \n",
      "\n",
      "[1m 8s (200 2%) 2.3825]\n",
      "Whams thew the rase thjy mades thesterle nakes waI ciicrucs baves, hy\n",
      "chest maltes? ath onhs maenild t \n",
      "\n",
      "[1m 42s (300 3%) 2.0130]\n",
      "What, her is the wan wee thee. Sor hofs siive.\n",
      "\n",
      "FEM:\n",
      "In to to me it the net to of so in the, epre the  \n",
      "\n",
      "[2m 17s (400 4%) 2.0316]\n",
      "Whis sher Anlighth do highitth for teav\n",
      "That be drake sas mand hin me way a frows teat, the she of war \n",
      "\n",
      "[2m 51s (500 5%) 2.3362]\n",
      "Wher a wave are comzanss.\n",
      "\n",
      "BFId sarse that haes, doth.\n",
      "\n",
      "Furrer your mang:\n",
      "Lathou morks your leark, Fro \n",
      "\n",
      "[3m 26s (600 6%) 2.0570]\n",
      "Whis theed fable sursers of this sorn of to uch\n",
      "tor ploness thubere\n",
      "hould for bould worgerarang toor n \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-882517906776>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_training_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mloss_avg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-a62e78e79030>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(inp, target)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 128\n",
    "n_layers = 3\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder.to(device)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  inputs, labels = random_training_set()\n",
    "  inputs, labels = inputs.to(device), labels.to(device)\n",
    "  loss = train(inputs, labels)       \n",
    "  loss_avg += loss\n",
    "\n",
    "  if epoch % print_every == 0:\n",
    "    print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "    print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "  if epoch % plot_every == 0:\n",
    "    all_losses.append(loss_avg / plot_every)\n",
    "    loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "colab_type": "code",
    "id": "ZAqSa7Hlewab",
    "outputId": "b0898f10-3f39-4cef-d99a-8ad5201e0b10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f90c5c5b550>]"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "hW7VKVTdyu7s",
    "outputId": "56f74b0e-66eb-4334-d596-c93345daf403"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The carses; and vator't,\n",
      "And it loldine: torile's and him and fortone-doodoss that is\n",
      "Thou come I trone, and indinderenes hoth porn to arn mine.\n",
      "\n",
      "ALGUDIMY ONOTE:\n",
      "Covee plat!\n",
      "\n",
      "RITO\n",
      "And to to that my my k\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "ldSZxXzN-6jc",
    "outputId": "43922232-c74e-46e1-efe0-d4e1483ad91d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mand the mare the mand the mand the rise to songer the come to doth the doth in to songer the the the may the the come to songer the come to songer the the the come.\n",
      "\n",
      "IOLIO:\n",
      "I and so so in the mand \n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "CvoXJRL0_A-i",
    "outputId": "073dbfc5-451c-462b-bfa5-3a1043bf7cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thall tade vOnquarge!wo,\n",
      "Theyousgy. I do qundale, it lither\n",
      "Dowhcm extaje i greplyall.\n",
      "Gowoa tal tished isastierim.\n",
      "'Ote, on tunkerex, s.\n",
      "It my your, you wrostiever,\n",
      "And a mist, nless, anl is let'this J\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "85giu5Ov_Cc0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of RNN_NLP_IndabaX_Kigali_Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
